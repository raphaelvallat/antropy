<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>antropy.entropy &#8212; antropy 0.1.9 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap-sphinx.css?v=9f61996d" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/style.css?v=917eae1c" />
    <script src="../../_static/documentation_options.js?v=8618f531"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=fd10adb8"></script>
    <link rel="icon" href="../../_static/antropy.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="../../_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          antropy</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1.9</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../api.html">API</a></li>
                <li><a href="../../changelog.html">What's new</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <h1>Source code for antropy.entropy</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Entropy functions&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">factorial</span><span class="p">,</span> <span class="n">log</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span> <span class="n">types</span>
<span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="kn">import</span> <span class="n">periodogram</span><span class="p">,</span> <span class="n">welch</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KDTree</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">_embed</span><span class="p">,</span> <span class="n">_xlogx</span>

<span class="nb">all</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;perm_entropy&quot;</span><span class="p">,</span>
    <span class="s2">&quot;spectral_entropy&quot;</span><span class="p">,</span>
    <span class="s2">&quot;svd_entropy&quot;</span><span class="p">,</span>
    <span class="s2">&quot;app_entropy&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sample_entropy&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lziv_complexity&quot;</span><span class="p">,</span>
    <span class="s2">&quot;num_zerocross&quot;</span><span class="p">,</span>
    <span class="s2">&quot;hjorth_params&quot;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="perm_entropy">
<a class="viewcode-back" href="../../generated/antropy.perm_entropy.html#antropy.perm_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">perm_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Permutation Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times)</span>
<span class="sd">    order : int</span>
<span class="sd">        Order of permutation entropy. Default is 3.</span>
<span class="sd">    delay : int, list, np.ndarray or range</span>
<span class="sd">        Time delay (lag). Default is 1. If multiple values are passed</span>
<span class="sd">        (e.g. [1, 2, 3]), AntroPy will calculate the average permutation</span>
<span class="sd">        entropy across all these delays.</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, divide by log2(order!) to normalize the entropy between 0</span>
<span class="sd">        and 1. Otherwise, return the permutation entropy in bit.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pe : float</span>
<span class="sd">        Permutation Entropy.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The permutation entropy is a complexity measure for time-series first</span>
<span class="sd">    introduced by Bandt and Pompe in 2002.</span>

<span class="sd">    The permutation entropy of a signal :math:`x` is defined as:</span>

<span class="sd">    .. math:: H = -\\sum p(\\pi)\\log_2(\\pi)</span>

<span class="sd">    where the sum runs over all :math:`n!` permutations :math:`\\pi` of order</span>
<span class="sd">    :math:`n`. This is the information contained in comparing :math:`n`</span>
<span class="sd">    consecutive values of the time series. It is clear that</span>
<span class="sd">    :math:`0 ≤ H (n) ≤ \\log_2(n!)` where the lower bound is attained for an</span>
<span class="sd">    increasing or decreasing sequence of values, and the upper bound for a</span>
<span class="sd">    completely random system where all :math:`n!` possible permutations appear</span>
<span class="sd">    with the same probability.</span>

<span class="sd">    The embedded matrix :math:`Y` is created by:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(i)=[x_i,x_{i+\\text{delay}}, ...,x_{i+(\\text{order}-1) *</span>
<span class="sd">        \\text{delay}}]</span>

<span class="sd">    .. math:: Y=[y(1),y(2),...,y(N-(\\text{order}-1))*\\text{delay})]^T</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a</span>
<span class="sd">    natural complexity measure for time series.&quot; Physical review letters</span>
<span class="sd">    88.17 (2002): 174102.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Permutation entropy with order 2</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; import stochastic.processes.noise as sn</span>
<span class="sd">    &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]</span>
<span class="sd">    &gt;&gt;&gt; # Return a value in bit between 0 and log2(factorial(order))</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.perm_entropy(x, order=2):.4f}&quot;)</span>
<span class="sd">    0.9183</span>

<span class="sd">    Normalized permutation entropy with order 3</span>

<span class="sd">    &gt;&gt;&gt; # Return a value comprised between 0 and 1.</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.perm_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.5888</span>

<span class="sd">    Fractional Gaussian noise with H = 0.5, averaged across multiple delays</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.perm_entropy(x, delay=[1, 2, 3], normalize=True):.4f}&quot;)</span>
<span class="sd">    0.9999</span>

<span class="sd">    Fractional Gaussian noise with H = 0.1, averaged across multiple delays</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.perm_entropy(x, delay=[1, 2, 3], normalize=True):.4f}&quot;)</span>
<span class="sd">    0.9986</span>

<span class="sd">    Random</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.perm_entropy(rng.random(1000), normalize=True):.4f}&quot;)</span>
<span class="sd">    0.9997</span>

<span class="sd">    Pure sine wave</span>

<span class="sd">    &gt;&gt;&gt; x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.perm_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.4463</span>

<span class="sd">    Linearly-increasing time-series</span>

<span class="sd">    &gt;&gt;&gt; x = np.arange(1000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.perm_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    -0.0000</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If multiple delay are passed, return the average across all d</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">delay</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">range</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">perm_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">delay</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ran_order</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
    <span class="n">hashmult</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">ran_order</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">delay</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;delay must be greater than zero.&quot;</span>
    <span class="c1"># Embed x and sort the order of permutations</span>
    <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">)</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;quicksort&quot;</span><span class="p">)</span>
    <span class="c1"># Associate unique integer to each permutations</span>
    <span class="n">hashval</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">sorted_idx</span><span class="p">,</span> <span class="n">hashmult</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Return the counts</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">hashval</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Use np.true_divide for Python 2 compatibility</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">true_divide</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="o">-</span><span class="n">_xlogx</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">pe</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">factorial</span><span class="p">(</span><span class="n">order</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pe</span></div>



<div class="viewcode-block" id="spectral_entropy">
<a class="viewcode-back" href="../../generated/antropy.spectral_entropy.html#antropy.spectral_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">spectral_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sf</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;fft&quot;</span><span class="p">,</span> <span class="n">nperseg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Spectral Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        1D or N-D data.</span>
<span class="sd">    sf : float</span>
<span class="sd">        Sampling frequency, in Hz.</span>
<span class="sd">    method : str</span>
<span class="sd">        Spectral estimation method:</span>

<span class="sd">        * ``&#39;fft&#39;`` : Fourier Transform (:py:func:`scipy.signal.periodogram`)</span>
<span class="sd">        * ``&#39;welch&#39;`` : Welch periodogram (:py:func:`scipy.signal.welch`)</span>
<span class="sd">    nperseg : int or None</span>
<span class="sd">        Length of each FFT segment for Welch method.</span>
<span class="sd">        If None (default), uses scipy default of 256 samples.</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, divide by log2(psd.size) to normalize the spectral entropy</span>
<span class="sd">        between 0 and 1. Otherwise, return the spectral entropy in bit.</span>
<span class="sd">    axis : int</span>
<span class="sd">        The axis along which the entropy is calculated. Default is -1 (last).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    se : float</span>
<span class="sd">        Spectral Entropy</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Spectral Entropy is defined to be the Shannon entropy of the power</span>
<span class="sd">    spectral density (PSD) of the data:</span>

<span class="sd">    .. math:: H(x, sf) =  -\\sum_{f=0}^{f_s/2} P(f) \\log_2[P(f)]</span>

<span class="sd">    Where :math:`P` is the normalised PSD, and :math:`f_s` is the sampling</span>
<span class="sd">    frequency.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    - Inouye, T. et al. (1991). Quantification of EEG irregularity by</span>
<span class="sd">      use of the entropy of the power spectrum. Electroencephalography</span>
<span class="sd">      and clinical neurophysiology, 79(3), 204-210.</span>

<span class="sd">    - https://en.wikipedia.org/wiki/Spectral_density</span>

<span class="sd">    - https://en.wikipedia.org/wiki/Welch%27s_method</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Spectral entropy of a pure sine using FFT</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; sf, f, dur = 100, 1, 4</span>
<span class="sd">    &gt;&gt;&gt; N = sf * dur # Total number of discrete samples</span>
<span class="sd">    &gt;&gt;&gt; t = np.arange(N) / sf # Time vector</span>
<span class="sd">    &gt;&gt;&gt; x = np.sin(2 * np.pi * f * t)</span>
<span class="sd">    &gt;&gt;&gt; np.round(ant.spectral_entropy(x, sf, method=&#39;fft&#39;), 2)</span>
<span class="sd">    0.0</span>

<span class="sd">    Spectral entropy of a random signal using Welch&#39;s method</span>

<span class="sd">    &gt;&gt;&gt; np.random.seed(42)</span>
<span class="sd">    &gt;&gt;&gt; x = np.random.rand(3000)</span>
<span class="sd">    &gt;&gt;&gt; ant.spectral_entropy(x, sf=100, method=&#39;welch&#39;)</span>
<span class="sd">    6.98004566237139</span>

<span class="sd">    Normalized spectral entropy</span>

<span class="sd">    &gt;&gt;&gt; ant.spectral_entropy(x, sf=100, method=&#39;welch&#39;, normalize=True)</span>
<span class="sd">    0.9955526198316073</span>

<span class="sd">    Normalized spectral entropy of 2D data</span>

<span class="sd">    &gt;&gt;&gt; np.random.seed(42)</span>
<span class="sd">    &gt;&gt;&gt; x = np.random.normal(size=(4, 3000))</span>
<span class="sd">    &gt;&gt;&gt; np.round(ant.spectral_entropy(x, sf=100, normalize=True), 4)</span>
<span class="sd">    array([0.9464, 0.9428, 0.9431, 0.9417])</span>

<span class="sd">    Fractional Gaussian noise with H = 0.5</span>

<span class="sd">    &gt;&gt;&gt; import stochastic.processes.noise as sn</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.spectral_entropy(x, sf=100, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.9505</span>

<span class="sd">    Fractional Gaussian noise with H = 0.9</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.spectral_entropy(x, sf=100, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.8477</span>

<span class="sd">    Fractional Gaussian noise with H = 0.1</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.spectral_entropy(x, sf=100, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.9248</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Compute and normalize power spectrum</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;fft&quot;</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">psd</span> <span class="o">=</span> <span class="n">periodogram</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;welch&quot;</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">psd</span> <span class="o">=</span> <span class="n">welch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sf</span><span class="p">,</span> <span class="n">nperseg</span><span class="o">=</span><span class="n">nperseg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">psd_norm</span> <span class="o">=</span> <span class="n">psd</span> <span class="o">/</span> <span class="n">psd</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="o">-</span><span class="n">_xlogx</span><span class="p">(</span><span class="n">psd_norm</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">se</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">psd_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">se</span></div>



<div class="viewcode-block" id="svd_entropy">
<a class="viewcode-back" href="../../generated/antropy.svd_entropy.html#antropy.svd_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">svd_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Singular Value Decomposition entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times)</span>
<span class="sd">    order : int</span>
<span class="sd">        Order of SVD entropy (= length of the embedding dimension).</span>
<span class="sd">        Default is 3.</span>
<span class="sd">    delay : int</span>
<span class="sd">        Time delay (lag). Default is 1.</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, divide by log2(order!) to normalize the entropy between 0</span>
<span class="sd">        and 1. Otherwise, return the permutation entropy in bit.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    svd_e : float</span>
<span class="sd">        SVD Entropy</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    SVD entropy is an indicator of the number of eigenvectors that are needed</span>
<span class="sd">    for an adequate explanation of the data set. In other words, it measures</span>
<span class="sd">    the dimensionality of the data.</span>

<span class="sd">    The SVD entropy of a signal :math:`x` is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        H = -\\sum_{i=1}^{M} \\overline{\\sigma}_i log_2(\\overline{\\sigma}_i)</span>

<span class="sd">    where :math:`M` is the number of singular values of the embedded matrix</span>
<span class="sd">    :math:`Y` and :math:`\\sigma_1, \\sigma_2, ..., \\sigma_M` are the</span>
<span class="sd">    normalized singular values of :math:`Y`.</span>

<span class="sd">    The embedded matrix :math:`Y` is created by:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(i)=[x_i,x_{i+\\text{delay}}, ...,x_{i+(\\text{order}-1) *</span>
<span class="sd">        \\text{delay}}]</span>

<span class="sd">    .. math:: Y=[y(1),y(2),...,y(N-(\\text{order}-1))*\\text{delay})]^T</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    SVD entropy with order 2</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; import stochastic.processes.noise as sn</span>
<span class="sd">    &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]</span>
<span class="sd">    &gt;&gt;&gt; # Return a value in bit between 0 and log2(factorial(order))</span>
<span class="sd">    &gt;&gt;&gt; print(ant.svd_entropy(x, order=2))</span>
<span class="sd">    0.7618909465130066</span>

<span class="sd">    Normalized SVD entropy with order 3</span>

<span class="sd">    &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]</span>
<span class="sd">    &gt;&gt;&gt; # Return a value comprised between 0 and 1.</span>
<span class="sd">    &gt;&gt;&gt; print(ant.svd_entropy(x, order=3, normalize=True))</span>
<span class="sd">    0.6870083043946692</span>

<span class="sd">    Fractional Gaussian noise with H = 0.5</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.svd_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    1.0000</span>

<span class="sd">    Fractional Gaussian noise with H = 0.9</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.svd_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.9080</span>

<span class="sd">    Fractional Gaussian noise with H = 0.1</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.svd_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.9637</span>

<span class="sd">    Random</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.svd_entropy(rng.random(1000), normalize=True):.4f}&quot;)</span>
<span class="sd">    0.8527</span>

<span class="sd">    Pure sine wave</span>

<span class="sd">    &gt;&gt;&gt; x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.svd_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.1775</span>

<span class="sd">    Linearly-increasing time-series</span>

<span class="sd">    &gt;&gt;&gt; x = np.arange(1000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.svd_entropy(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.0053</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mat</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Normalize the singular values</span>
    <span class="n">W</span> <span class="o">/=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">svd_e</span> <span class="o">=</span> <span class="o">-</span><span class="n">_xlogx</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">svd_e</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">svd_e</span></div>



<span class="k">def</span> <span class="nf">_app_samp_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;chebyshev&quot;</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Utility function for `app_entropy`` and `sample_entropy`.&quot;&quot;&quot;</span>
    <span class="n">_all_metrics</span> <span class="o">=</span> <span class="n">KDTree</span><span class="o">.</span><span class="n">valid_metrics</span>
    <span class="n">_all_metrics</span> <span class="o">=</span> <span class="n">_all_metrics</span><span class="p">()</span> <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">_all_metrics</span><span class="p">)</span> <span class="k">else</span> <span class="n">_all_metrics</span>
    <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_all_metrics</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The given metric (</span><span class="si">%s</span><span class="s2">) is not valid. The valid &quot;</span>
            <span class="s2">&quot;metric names are: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">_all_metrics</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># compute phi(order, r)</span>
    <span class="n">_emb_data1</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">approximate</span><span class="p">:</span>
        <span class="n">emb_data1</span> <span class="o">=</span> <span class="n">_emb_data1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_data1</span> <span class="o">=</span> <span class="n">_emb_data1</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">count1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">KDTree</span><span class="p">(</span><span class="n">emb_data1</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span>
        <span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">emb_data1</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">count_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># compute phi(order + 1, r)</span>
    <span class="n">emb_data2</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">count2</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">KDTree</span><span class="p">(</span><span class="n">emb_data2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span>
        <span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">emb_data2</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">count_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">approximate</span><span class="p">:</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">count1</span> <span class="o">/</span> <span class="n">emb_data1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">count2</span> <span class="o">/</span> <span class="n">emb_data2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">count1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">emb_data1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">count2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">emb_data2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">phi</span>


<span class="nd">@jit</span><span class="p">(</span>
    <span class="p">(</span><span class="n">types</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span><span class="n">types</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">readonly</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">types</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span>
    <span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">_numba_sampen</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">order</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast evaluation of the sample entropy using Numba.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">sequence</span><span class="o">.</span><span class="n">size</span>
    <span class="c1"># sequence = sequence.tolist()</span>

    <span class="n">numerator</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="n">order</span><span class="p">):</span>
        <span class="n">n_numerator</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="n">order</span><span class="p">]</span> <span class="o">-</span> <span class="n">sequence</span><span class="p">[</span><span class="n">order</span> <span class="o">+</span> <span class="n">offset</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">r</span><span class="p">)</span>
        <span class="n">n_denominator</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">):</span>
            <span class="n">n_numerator</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">sequence</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">offset</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">r</span>
            <span class="n">n_denominator</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">sequence</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">offset</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">r</span>

        <span class="k">if</span> <span class="n">n_numerator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">numerator</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">n_denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">denominator</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">prev_in_diff</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="n">order</span><span class="p">]</span> <span class="o">-</span> <span class="n">sequence</span><span class="p">[</span><span class="n">offset</span> <span class="o">+</span> <span class="n">order</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">r</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="n">offset</span> <span class="o">-</span> <span class="n">order</span><span class="p">):</span>
            <span class="n">out_diff</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">sequence</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">offset</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">r</span><span class="p">)</span>
            <span class="n">in_diff</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">order</span><span class="p">]</span> <span class="o">-</span> <span class="n">sequence</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">order</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">r</span><span class="p">)</span>
            <span class="n">n_numerator</span> <span class="o">+=</span> <span class="n">in_diff</span> <span class="o">-</span> <span class="n">out_diff</span>
            <span class="n">n_denominator</span> <span class="o">+=</span> <span class="n">prev_in_diff</span> <span class="o">-</span> <span class="n">out_diff</span>
            <span class="n">prev_in_diff</span> <span class="o">=</span> <span class="n">in_diff</span>

            <span class="k">if</span> <span class="n">n_numerator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">numerator</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">n_denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">denominator</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>  <span class="c1"># use 0/0 == 0</span>
    <span class="k">elif</span> <span class="n">numerator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">)</span>


<div class="viewcode-block" id="app_entropy">
<a class="viewcode-back" href="../../generated/antropy.app_entropy.html#antropy.app_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">app_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;chebyshev&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Approximate Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times).</span>
<span class="sd">    order : int</span>
<span class="sd">        Embedding dimension. Default is 2.</span>
<span class="sd">    tolerance : float</span>
<span class="sd">        Tolerance value for acceptance of the template vector. Default is 0.2</span>
<span class="sd">        times the standard deviation of x.</span>
<span class="sd">    metric : str</span>
<span class="sd">        Name of the distance metric function used with</span>
<span class="sd">        :py:class:`sklearn.neighbors.KDTree`. Default is to use the</span>
<span class="sd">        `Chebyshev &lt;https://en.wikipedia.org/wiki/Chebyshev_distance&gt;`_</span>
<span class="sd">        distance.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ae : float</span>
<span class="sd">        Approximate Entropy.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Approximate entropy is a technique used to quantify the amount of</span>
<span class="sd">    regularity and the unpredictability of fluctuations over time-series data.</span>
<span class="sd">    Smaller values indicates that the data is more regular and predictable.</span>

<span class="sd">    The default tolerance value (:math:`r`) is set to :math:`0.2 * \\text{std}(x)`.</span>

<span class="sd">    Code adapted from the `mne-features &lt;https://mne.tools/mne-features/&gt;`_</span>
<span class="sd">    package by Jean-Baptiste Schiratti and Alexandre Gramfort.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Richman, J. S. et al. (2000). Physiological time-series analysis</span>
<span class="sd">    using approximate entropy and sample entropy. American Journal of</span>
<span class="sd">    Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</span>

<span class="sd">    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Fractional Gaussian noise with H = 0.5</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; import stochastic.processes.noise as sn</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.app_entropy(x, order=2):.4f}&quot;)</span>
<span class="sd">    2.1958</span>

<span class="sd">    Same with order = 3 and metric = &#39;euclidean&#39;</span>

<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.app_entropy(x, order=3, metric=&#39;euclidean&#39;):.4f}&quot;)</span>
<span class="sd">    1.5120</span>

<span class="sd">    Fractional Gaussian noise with H = 0.9</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.app_entropy(x):.4f}&quot;)</span>
<span class="sd">    1.9681</span>

<span class="sd">    Fractional Gaussian noise with H = 0.1</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.app_entropy(x):.4f}&quot;)</span>
<span class="sd">    2.0906</span>

<span class="sd">    Random</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.app_entropy(rng.random(1000)):.4f}&quot;)</span>
<span class="sd">    1.8177</span>

<span class="sd">    Pure sine wave</span>

<span class="sd">    &gt;&gt;&gt; x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.app_entropy(x):.4f}&quot;)</span>
<span class="sd">    0.2009</span>

<span class="sd">    Linearly-increasing time-series</span>

<span class="sd">    &gt;&gt;&gt; x = np.arange(1000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.app_entropy(x):.4f}&quot;)</span>
<span class="sd">    -0.0010</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># define r</span>
    <span class="k">if</span> <span class="n">tolerance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tolerance</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">tolerance</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">_app_samp_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></div>



<div class="viewcode-block" id="sample_entropy">
<a class="viewcode-back" href="../../generated/antropy.sample_entropy.html#antropy.sample_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">sample_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;chebyshev&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times).</span>
<span class="sd">    order : int</span>
<span class="sd">        Embedding dimension. Default is 2.</span>
<span class="sd">    tolerance : float</span>
<span class="sd">        Tolerance value for acceptance of the template vector. Default is 0.2</span>
<span class="sd">        times the standard deviation of x.</span>
<span class="sd">    metric : str</span>
<span class="sd">        Name of the distance metric function used with</span>
<span class="sd">        :py:class:`sklearn.neighbors.KDTree`. Default is to use the</span>
<span class="sd">        `Chebyshev &lt;https://en.wikipedia.org/wiki/Chebyshev_distance&gt;`_</span>
<span class="sd">        distance.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    se : float</span>
<span class="sd">        Sample Entropy.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Sample entropy is a modification of approximate entropy, used for assessing</span>
<span class="sd">    the complexity of physiological time-series signals. It has two advantages</span>
<span class="sd">    over approximate entropy: data length independence and a relatively</span>
<span class="sd">    trouble-free implementation. Large values indicate high complexity whereas</span>
<span class="sd">    smaller values characterize more self-similar and regular signals.</span>

<span class="sd">    The sample entropy of a signal :math:`x` is defined as:</span>

<span class="sd">    .. math:: H(x, m, r) = -\\log\\frac{C(m + 1, r)}{C(m, r)}</span>

<span class="sd">    where :math:`m` is the embedding dimension (= order), :math:`r` is</span>
<span class="sd">    the radius of the neighbourhood (default = :math:`0.2 * \\text{std}(x)`),</span>
<span class="sd">    :math:`C(m + 1, r)` is the number of embedded vectors of length</span>
<span class="sd">    :math:`m + 1` having a</span>
<span class="sd">    `Chebyshev distance &lt;https://en.wikipedia.org/wiki/Chebyshev_distance&gt;`_</span>
<span class="sd">    inferior to :math:`r` and :math:`C(m, r)` is the number of embedded</span>
<span class="sd">    vectors of length :math:`m` having a Chebyshev distance inferior to</span>
<span class="sd">    :math:`r`.</span>

<span class="sd">    Note that if ``metric == &#39;chebyshev&#39;`` and ``len(x) &lt; 5000`` points,</span>
<span class="sd">    then the sample entropy is computed using a fast custom Numba script.</span>
<span class="sd">    For other distance metric or longer time-series, the sample entropy is</span>
<span class="sd">    computed using a code from the</span>
<span class="sd">    `mne-features &lt;https://mne.tools/mne-features/&gt;`_ package by Jean-Baptiste</span>
<span class="sd">    Schiratti and Alexandre Gramfort (requires sklearn).</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Richman, J. S. et al. (2000). Physiological time-series analysis</span>
<span class="sd">    using approximate entropy and sample entropy. American Journal of</span>
<span class="sd">    Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</span>

<span class="sd">    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Fractional Gaussian noise with H = 0.5</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; import stochastic.processes.noise as sn</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.sample_entropy(x, order=2):.4f}&quot;)</span>
<span class="sd">    2.1819</span>

<span class="sd">    Same with order = 3 and using the Euclidean distance</span>

<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.sample_entropy(x, order=3, metric=&#39;euclidean&#39;):.4f}&quot;)</span>
<span class="sd">    2.6806</span>

<span class="sd">    Fractional Gaussian noise with H = 0.9</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.sample_entropy(x):.4f}&quot;)</span>
<span class="sd">    1.9078</span>

<span class="sd">    Fractional Gaussian noise with H = 0.1</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.sample_entropy(x):.4f}&quot;)</span>
<span class="sd">    2.0555</span>

<span class="sd">    Random</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.sample_entropy(rng.random(1000)):.4f}&quot;)</span>
<span class="sd">    2.2017</span>

<span class="sd">    Pure sine wave</span>

<span class="sd">    &gt;&gt;&gt; x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.sample_entropy(x):.4f}&quot;)</span>
<span class="sd">    0.1633</span>

<span class="sd">    Linearly-increasing time-series</span>

<span class="sd">    &gt;&gt;&gt; x = np.arange(1000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.sample_entropy(x):.4f}&quot;)</span>
<span class="sd">    -0.0000</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># define r</span>
    <span class="k">if</span> <span class="n">tolerance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tolerance</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">tolerance</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;chebyshev&quot;</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">&lt;</span> <span class="mi">5000</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_numba_sampen</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">_app_samp_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span></div>



<span class="nd">@jit</span><span class="p">(</span><span class="s2">&quot;uint32(uint32[:])&quot;</span><span class="p">,</span> <span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_lz_complexity</span><span class="p">(</span><span class="n">binary_string</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal Numba implementation of the Lempel-Ziv (LZ) complexity.</span>
<span class="sd">    https://github.com/Naereen/Lempel-Ziv_Complexity/blob/master/src/lziv_complexity.py</span>
<span class="sd">    - Updated with strict integer typing instead of strings</span>
<span class="sd">    - Slight restructuring based on Yacine Mahdid&#39;s notebook:</span>
<span class="sd">    https://github.com/BIAPT/Notebooks/blob/master/features/Lempel-Ziv%20Complexity.ipynb</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize variables</span>
    <span class="n">complexity</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">prefix_len</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">len_substring</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">max_len_substring</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">pointer</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Iterate until the entire string has not been parsed</span>
    <span class="k">while</span> <span class="n">prefix_len</span> <span class="o">+</span> <span class="n">len_substring</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">binary_string</span><span class="p">):</span>
        <span class="c1"># Given a prefix length, find the largest substring</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">binary_string</span><span class="p">[</span><span class="n">pointer</span> <span class="o">+</span> <span class="n">len_substring</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">binary_string</span><span class="p">[</span><span class="n">prefix_len</span> <span class="o">+</span> <span class="n">len_substring</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># noqa: W503</span>
        <span class="p">):</span>
            <span class="n">len_substring</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># increase the length of the substring</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_len_substring</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">len_substring</span><span class="p">,</span> <span class="n">max_len_substring</span><span class="p">)</span>
            <span class="n">pointer</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># Since all pointers have been scanned, pick largest as the jump</span>
            <span class="c1"># size</span>
            <span class="k">if</span> <span class="n">pointer</span> <span class="o">==</span> <span class="n">prefix_len</span><span class="p">:</span>
                <span class="c1"># Increment complexity</span>
                <span class="n">complexity</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="c1"># Set prefix length to the max substring size found so far</span>
                <span class="c1"># (jump size)</span>
                <span class="n">prefix_len</span> <span class="o">+=</span> <span class="n">max_len_substring</span>
                <span class="c1"># Reset pointer and max substring size</span>
                <span class="n">pointer</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">max_len_substring</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># Reset length of current substring</span>
            <span class="n">len_substring</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Check if final iteration occurred in the middle of a substring</span>
    <span class="k">if</span> <span class="n">len_substring</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">complexity</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">complexity</span>


<div class="viewcode-block" id="lziv_complexity">
<a class="viewcode-back" href="../../generated/antropy.lziv_complexity.html#antropy.lziv_complexity">[docs]</a>
<span class="k">def</span> <span class="nf">lziv_complexity</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Lempel-Ziv (LZ) complexity of (binary) sequence.</span>

<span class="sd">    .. versionadded:: 0.1.1</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    sequence : str or array</span>
<span class="sd">        A sequence of character, e.g. ``&#39;1001111011000010&#39;``,</span>
<span class="sd">        ``[0, 1, 0, 1, 1]``, or ``&#39;Hello World!&#39;``.</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If ``True``, returns the normalized LZ (see Notes).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    lz : int or float</span>
<span class="sd">        LZ complexity, which corresponds to the number of different</span>
<span class="sd">        substrings encountered as the stream is viewed from the</span>
<span class="sd">        beginning to the end. If ``normalize=False``, the output is an</span>
<span class="sd">        integer (counts), otherwise the output is a float.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    LZ complexity is defined as the number of different substrings encountered</span>
<span class="sd">    as the sequence is viewed from begining to the end.</span>

<span class="sd">    Although the raw LZ is an important complexity indicator, it is heavily</span>
<span class="sd">    influenced by sequence length (longer sequence will result in higher LZ).</span>
<span class="sd">    Zhang and colleagues (2009) have therefore proposed the normalized LZ,</span>
<span class="sd">    which is defined by</span>

<span class="sd">    .. math:: \\text{LZn} = \\frac{\\text{LZ}}{(n / \\log_b{n})}</span>

<span class="sd">    where :math:`n` is the length of the sequence and :math:`b` the number of</span>
<span class="sd">    unique characters in the sequence.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    * Lempel, A., &amp; Ziv, J. (1976). On the Complexity of Finite Sequences.</span>
<span class="sd">      IEEE Transactions on Information Theory / Professional Technical</span>
<span class="sd">      Group on Information Theory, 22(1), 75–81.</span>
<span class="sd">      https://doi.org/10.1109/TIT.1976.1055501</span>
<span class="sd">    * Zhang, Y., Hao, J., Zhou, C., &amp; Chang, K. (2009). Normalized</span>
<span class="sd">      Lempel-Ziv complexity and its application in bio-sequence analysis.</span>
<span class="sd">      Journal of Mathematical Chemistry, 46(4), 1203–1212.</span>
<span class="sd">      https://doi.org/10.1007/s10910-008-9512-2</span>
<span class="sd">    * https://en.wikipedia.org/wiki/Lempel-Ziv_complexity</span>
<span class="sd">    * https://github.com/Naereen/Lempel-Ziv_Complexity</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from antropy import lziv_complexity</span>
<span class="sd">    &gt;&gt;&gt; # Substrings = 1 / 0 / 01 / 1110 / 1100 / 0010</span>
<span class="sd">    &gt;&gt;&gt; s = &#39;1001111011000010&#39;</span>
<span class="sd">    &gt;&gt;&gt; lziv_complexity(s)</span>
<span class="sd">    6</span>

<span class="sd">    Using a list of integer / boolean instead of a string</span>

<span class="sd">    &gt;&gt;&gt; # 1 / 0 / 10</span>
<span class="sd">    &gt;&gt;&gt; lziv_complexity([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])</span>
<span class="sd">    3</span>

<span class="sd">    With normalization</span>

<span class="sd">    &gt;&gt;&gt; lziv_complexity(s, normalize=True)</span>
<span class="sd">    1.5</span>

<span class="sd">    This function also works with characters and words</span>

<span class="sd">    &gt;&gt;&gt; s = &#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;</span>
<span class="sd">    &gt;&gt;&gt; lziv_complexity(s), lziv_complexity(s, normalize=True)</span>
<span class="sd">    (26, 1.0)</span>

<span class="sd">    &gt;&gt;&gt; s = &#39;HELLO WORLD! HELLO WORLD! HELLO WORLD! HELLO WORLD!&#39;</span>
<span class="sd">    &gt;&gt;&gt; lziv_complexity(s), lziv_complexity(s, normalize=True)</span>
<span class="sd">    (11, 0.38596001132145313)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sequence</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="ow">in</span> <span class="s2">&quot;bfi&quot;</span><span class="p">:</span>
            <span class="c1"># Convert [True, False] or [1., 0.] to [1, 0]</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">sequence</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint32&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Treat as numpy array of strings</span>
            <span class="c1"># Map string characters to utf-8 integer representation</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">))),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint32&quot;</span><span class="p">)</span>
            <span class="c1"># Can&#39;t preallocate length (by specifying count) due to string</span>
            <span class="c1"># concatenation</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="n">sequence</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint32&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="c1"># 1) Timmermann et al. 2019</span>
        <span class="c1"># The sequence is randomly shuffled, and the normalized LZ</span>
        <span class="c1"># is calculated as the ratio of the LZ of the original sequence</span>
        <span class="c1"># divided by the LZ of the randomly shuffled LZ. However, the final</span>
        <span class="c1"># output is dependent on the random seed.</span>
        <span class="c1"># sl_shuffled = list(s)</span>
        <span class="c1"># rng = np.random.RandomState(None)</span>
        <span class="c1"># rng.shuffle(sl_shuffled)</span>
        <span class="c1"># s_shuffled = &#39;&#39;.join(sl_shuffled)</span>
        <span class="c1"># return _lz_complexity(s) / _lz_complexity(s_shuffled)</span>
        <span class="c1"># 2) Zhang et al. 2009</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">base</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Number of unique characters</span>
        <span class="n">base</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">base</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">base</span>
        <span class="k">return</span> <span class="n">_lz_complexity</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">/</span> <span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">base</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_lz_complexity</span><span class="p">(</span><span class="n">s</span><span class="p">)</span></div>



<span class="c1">###############################################################################</span>
<span class="c1"># OTHER TIME-DOMAIN METRICS</span>
<span class="c1">###############################################################################</span>


<div class="viewcode-block" id="num_zerocross">
<a class="viewcode-back" href="../../generated/antropy.num_zerocross.html#antropy.num_zerocross">[docs]</a>
<span class="k">def</span> <span class="nf">num_zerocross</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of zero-crossings.</span>

<span class="sd">    .. versionadded: 0.1.3</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        1D or N-D data.</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, divide by the number of samples to normalize the output</span>
<span class="sd">        between 0 and 1. Otherwise, return the absolute number of zero</span>
<span class="sd">        crossings.</span>
<span class="sd">    axis : int</span>
<span class="sd">        The axis along which to perform the computation. Default is -1 (last).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    nzc : int or float</span>
<span class="sd">        Number of zero-crossings.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Simple examples</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; ant.num_zerocross([-1, 0, 1, 2, 3])</span>
<span class="sd">    1</span>

<span class="sd">    &gt;&gt;&gt; ant.num_zerocross([0, 0, 2, -1, 0, 1, 0, 2])</span>
<span class="sd">    2</span>

<span class="sd">    Number of zero crossings of a pure sine</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; sf, f, dur = 100, 1, 4</span>
<span class="sd">    &gt;&gt;&gt; N = sf * dur # Total number of discrete samples</span>
<span class="sd">    &gt;&gt;&gt; t = np.arange(N) / sf # Time vector</span>
<span class="sd">    &gt;&gt;&gt; x = np.sin(2 * np.pi * f * t)</span>
<span class="sd">    &gt;&gt;&gt; ant.num_zerocross(x)</span>
<span class="sd">    7</span>

<span class="sd">    Random 2D data</span>

<span class="sd">    &gt;&gt;&gt; np.random.seed(42)</span>
<span class="sd">    &gt;&gt;&gt; x = np.random.normal(size=(4, 3000))</span>
<span class="sd">    &gt;&gt;&gt; ant.num_zerocross(x)</span>
<span class="sd">    array([1499, 1528, 1547, 1457])</span>

<span class="sd">    Same but normalized by the number of samples</span>

<span class="sd">    &gt;&gt;&gt; np.round(ant.num_zerocross(x, normalize=True), 4)</span>
<span class="sd">    array([0.4997, 0.5093, 0.5157, 0.4857])</span>

<span class="sd">    Fractional Gaussian noise with H = 0.5</span>

<span class="sd">    &gt;&gt;&gt; import stochastic.processes.noise as sn</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.num_zerocross(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.4973</span>

<span class="sd">    Fractional Gaussian noise with H = 0.9</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.num_zerocross(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.2615</span>

<span class="sd">    Fractional Gaussian noise with H = 0.1</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;{ant.num_zerocross(x, normalize=True):.4f}&quot;)</span>
<span class="sd">    0.6451</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># https://stackoverflow.com/a/29674950/10581531</span>
    <span class="n">nzc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">signbit</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">nzc</span> <span class="o">=</span> <span class="n">nzc</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">nzc</span></div>



<div class="viewcode-block" id="hjorth_params">
<a class="viewcode-back" href="../../generated/antropy.hjorth_params.html#antropy.hjorth_params">[docs]</a>
<span class="k">def</span> <span class="nf">hjorth_params</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate Hjorth mobility and complexity on given axis.</span>

<span class="sd">    .. versionadded: 0.1.3</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        1D or N-D data.</span>
<span class="sd">    axis : int</span>
<span class="sd">        The axis along which to perform the computation. Default is -1 (last).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    mobility, complexity : float</span>
<span class="sd">        Mobility and complexity parameters.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Hjorth Parameters are indicators of statistical properties used in signal</span>
<span class="sd">    processing in the time domain introduced by Bo Hjorth in 1970. The</span>
<span class="sd">    parameters are activity, mobility, and complexity. EntroPy only returns the</span>
<span class="sd">    mobility and complexity parameters, since activity is simply the variance</span>
<span class="sd">    of :math:`x`, which can be computed easily with :py:func:`numpy.var`.</span>

<span class="sd">    The **mobility** parameter represents the mean frequency or the proportion</span>
<span class="sd">    of standard deviation of the power spectrum. This is defined as the square</span>
<span class="sd">    root of variance of the first derivative of :math:`x` divided by the</span>
<span class="sd">    variance of :math:`x`.</span>

<span class="sd">    The **complexity** gives an estimate of the bandwidth of the signal, which</span>
<span class="sd">    indicates the similarity of the shape of the signal to a pure sine wave</span>
<span class="sd">    (where the value converges to 1). Complexity is defined as the ratio of</span>
<span class="sd">    the mobility of the first derivative of :math:`x` to the mobility of</span>
<span class="sd">    :math:`x`.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    - https://en.wikipedia.org/wiki/Hjorth_parameters</span>
<span class="sd">    - https://doi.org/10.1016%2F0013-4694%2870%2990143-4</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Hjorth parameters of a pure sine</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import antropy as ant</span>
<span class="sd">    &gt;&gt;&gt; sf, f, dur = 100, 1, 4</span>
<span class="sd">    &gt;&gt;&gt; N = sf * dur # Total number of discrete samples</span>
<span class="sd">    &gt;&gt;&gt; t = np.arange(N) / sf # Time vector</span>
<span class="sd">    &gt;&gt;&gt; x = np.sin(2 * np.pi * f * t)</span>
<span class="sd">    &gt;&gt;&gt; np.round(ant.hjorth_params(x), 4)</span>
<span class="sd">    array([0.0627, 1.005 ])</span>

<span class="sd">    Random 2D data</span>

<span class="sd">    &gt;&gt;&gt; np.random.seed(42)</span>
<span class="sd">    &gt;&gt;&gt; x = np.random.normal(size=(4, 3000))</span>
<span class="sd">    &gt;&gt;&gt; mob, com = ant.hjorth_params(x)</span>
<span class="sd">    &gt;&gt;&gt; print(mob)</span>
<span class="sd">    [1.42145064 1.4339572  1.42186993 1.40587512]</span>

<span class="sd">    &gt;&gt;&gt; print(com)</span>
<span class="sd">    [1.21877527 1.21092261 1.217278   1.22623163]</span>

<span class="sd">    Fractional Gaussian noise with H = 0.5</span>

<span class="sd">    &gt;&gt;&gt; import stochastic.processes.noise as sn</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; np.round(ant.hjorth_params(x), 4)</span>
<span class="sd">    array([1.4073, 1.2283])</span>

<span class="sd">    Fractional Gaussian noise with H = 0.9</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; np.round(ant.hjorth_params(x), 4)</span>
<span class="sd">    array([0.8395, 1.9143])</span>

<span class="sd">    Fractional Gaussian noise with H = 0.1</span>

<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)</span>
<span class="sd">    &gt;&gt;&gt; x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)</span>
<span class="sd">    &gt;&gt;&gt; np.round(ant.hjorth_params(x), 4)</span>
<span class="sd">    array([1.6917, 1.0717])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Calculate derivatives</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">ddx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="c1"># Calculate variance</span>
    <span class="n">x_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>  <span class="c1"># = activity</span>
    <span class="n">dx_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">ddx_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">ddx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="c1"># Mobility and complexity</span>
    <span class="n">mob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dx_var</span> <span class="o">/</span> <span class="n">x_var</span><span class="p">)</span>
    <span class="n">com</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ddx_var</span> <span class="o">/</span> <span class="n">dx_var</span><span class="p">)</span> <span class="o">/</span> <span class="n">mob</span>
    <span class="k">return</span> <span class="n">mob</span><span class="p">,</span> <span class="n">com</span></div>

</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2018-2025, Raphael Vallat.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 7.3.7.<br/>
    </p>
  </div>
</footer>
  </body>
</html>